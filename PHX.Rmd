---
title: "Lab 05 - Predicting MHV Change"
author: "Johnny Dan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    theme: cerulean
    highlight: haddock
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set( echo = TRUE, message=F, warning=F, fig.width=10 )
```

```{r}
library( dplyr )
library( knitr )
library( pander )
library( stargazer )
library( scales )
library( geojsonio )   # read shapefiles
library( sp )          # work with shapefiles
library( sf )          # work with shapefiles - simple features format
library( mclust )      # cluster analysis 
library( tmap )        # theme maps
library( ggplot2 )     # graphing 
library( ggthemes )    # nice formats for ggplots
library( tidycensus )
library( cartogram )  # spatial maps w/ tract size bias reduction
library( maptools )   # spatial object manipulation 
library( corrplot )   # correlation plots 
library( tigris )
library(rgdal)

set.seed( 1234 )

census_api_key("904a150cb5cb82a7e89c173432778996874f9e6d")
```

```{r}
crosswalk <- read.csv( "https://raw.githubusercontent.com/DS4PS/cpp-529-master/master/data/cbsatocountycrosswalk.csv",  stringsAsFactors=F, colClasses="character" )

grep( "^MARICOPA", crosswalk$countyname, value=TRUE ) 

these.msp <- 
  crosswalk$countyname == grep( "^MARICOPA", crosswalk$countyname, value=TRUE ) 

these.fips <- crosswalk$fipscounty[ these.msp ]
these.fips <- na.omit( these.fips )

state.fips <- substr( these.fips, 1, 2 )
county.fips <- substr( these.fips, 3, 5 )

cbind( these.fips, state.fips, county.fips ) %>% pander()


# get the san diego data
phx.pop <- get_acs( 
  geography = "tract", 
  variables = "B01003_001", 
  state = state.fips, 
  county = county.fips,
  geometry = TRUE ) %>% 
  select( GEOID, estimate ) %>%
  rename( POP=estimate )

# recode the GEIOD variable to conform with the census data
# remove the leading zero
phx.pop$GEOID<-sub( ".","", phx.pop$GEOID )

URL <- "https://github.com/DS4PS/cpp-529-master/raw/master/data/ltdb_std_2010_sample.rds"
census.dat <- readRDS( gzcon( url( URL ) ) )

# merge the pop data for san diego with the census data
phxx <- merge( phx.pop, census.dat, by.x="GEOID", by.y="tractid" )

# make sure there are no empty polygons
phxx <- phxx[ ! st_is_empty( phxx ) , ]

# convert sf map object to an sp version
phxx.sp <- as_Spatial( phxx )

class( phxx.sp )

plot( phxx.sp )

# project map and remove empty tracts
phxx.sp <- spTransform( phxx.sp, CRS( "+init=epsg:3395" ) )
phxx.sp <- phxx.sp[ phxx.sp$POP != 0 & (! is.na( phxx.sp$POP ) ) , ]

phxx.sp$pop.w <- phxx.sp$POP / 10000
phx_dorling <- cartogram_dorling(x = phxx.sp, weight = "pop.w", k = 0.03)
plot(phx_dorling)

class(phx_dorling)

keep.these <- c("pnhwht12", "pnhblk12", "phisp12", "pntv12", "pfb12", "polang12", 
"phs12", "pcol12", "punemp12", "pflabf12", "pprof12", "pmanuf12", 
"pvet12", "psemp12", "hinc12", "incpc12", "ppov12", "pown12", 
"pvac12", "pmulti12", "mrent12", "mhmval12", "p30old12", "p10yrs12", 
"p18und12", "p60up12", "p75up12", "pmar12", "pwds12", "pfhh12")

d1 <- phx_dorling@data
d2 <- select( d1, keep.these )
d3 <- apply( d2, 2, scale )

fit <- Mclust( d3 )
phx_dorling$cluster <- fit$classification

URL1 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2000.rds"
d1 <- readRDS( gzcon( url( URL1 ) ) )

URL2 <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-2010.rds"
d2 <- readRDS( gzcon( url( URL2 ) ) )

URLmd <- "https://github.com/DS4PS/cpp-529-fall-2020/raw/main/LABS/data/rodeo/LTDB-META-DATA.rds"
md <- readRDS( gzcon( url( URLmd ) ) )

d1 <- select( d1, - year )
d2 <- select( d2, - year )

d <- merge( d1, d2, by="tractid" )
d <- merge( d, md, by="tractid" )

x <- d$tractid 

# remove non-numeric strings 
x <- gsub( "fips", "", x )
x <- gsub( "-", "", x )

x <- as.numeric( x )

d$tractid <- x 

phx_dorling <- merge( phx_dorling, d, by.x="GEOID", by.y="tractid", all.x=T )

# data frame and polygon ID standardization in case a tract was dropped and IDs don't match
row.ids <- sapply( slot( phx_dorling, "polygons" ), function(x) slot( x, "ID" ) )
row.names( phx_dorling ) <- row.ids

# project to standard lat-lon coordinate system 
phx_dorling <- spTransform( phx_dorling, CRS("+proj=longlat +datum=WGS84") )

# Save Dorling To File 
geojson_write( phx_dorling, file="phx.geojson", geometry="polygon" )
```
